{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIKI_CORPUS_PATH = './data/simplewiki-20171020-pages-articles-multistream.xml.bz2'\n",
    "WIKI_DICT_PATH = 'lda/id2word_wiki.txt'\n",
    "WIKI_BOW_FILE = 'lda/wiki_bow.mm'\n",
    "NO_BELOW = 20 # filter words that appear in less than this many documents\n",
    "NO_ABOVE_PCT = 0.1 # filter words that appear in more than this percent of documents\n",
    "\n",
    "LDA_SAVE_FILE = 'lda/lda_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50 or any(title.startswith(ns + ':') for ns in ignore_namespaces):\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield title, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [u'april', u'th', u'month', u'year', u'comes', u'march', u'months', u'days', u'april', u'begins']\n",
      "August [u'august', u'aug', u'th', u'month', u'year', u'gregorian', u'calendar', u'coming', u'july', u'september']\n",
      "Art [u'painting', u'renoir', u'work', u'art', u'art', u'creative', u'activity', u'people', u'people', u'called']\n",
      "A [u'page', u'letter', u'alphabet', u'indefinite', u'article', u'article', u'grammar', u'uses', u'disambiguation', u'thumb']\n",
      "Air [u'air', u'fan', u'air', u'air', u'earth', u'atmosphere', u'air', u'mixture', u'gases', u'dust']\n",
      "Autonomous communities of Spain [u'spain', u'divided', u'parts', u'called', u'autonomous', u'communities', u'autonomous', u'means', u'autonomous', u'communities']\n",
      "Alan Turing [u'statue', u'alan', u'turing', u'rebuild', u'machine', u'alan', u'turing', u'alan', u'mathison', u'turing']\n",
      "Alanis Morissette [u'alanis', u'nadine', u'morissette', u'born', u'june', u'grammy', u'award', u'winning', u'canadian', u'american']\n"
     ]
    }
   ],
   "source": [
    "# Print the article title and its first ten tokens as an example\n",
    "stream = iter_wiki(WIKI_CORPUS_PATH)\n",
    "for title, tokens in itertools.islice(iter_wiki(WIKI_CORPUS_PATH), 8):\n",
    "    print title, tokens[:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionary id2word using wikicorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_stream = (tokens for _, tokens in iter_wiki(WIKI_CORPUS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(156775 unique tokens: [u'fawn', u'\\u03c9\\u0431\\u0440\\u0430\\u0434\\u043e\\u0432\\u0430\\u043d\\u043d\\u0430\\u0467', u'vang', u'yollar\\u0131', u'idaira']...)\n",
      "INFO : adding document #20000 to Dictionary(232594 unique tokens: [u'biennials', u'sowela', u'tsukino', u'clottes', u'refreshable']...)\n",
      "INFO : adding document #30000 to Dictionary(292328 unique tokens: [u'biennials', u'sowela', u'tsukino', u'clottes', u'klatki']...)\n",
      "INFO : adding document #40000 to Dictionary(368454 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : adding document #50000 to Dictionary(416045 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : adding document #60000 to Dictionary(454336 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : built Dictionary(461803 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...) from 61418 documents (total 13008700 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 2.13 s, total: 4min 20s\n",
      "Wall time: 4min 20s\n",
      "Dictionary(461803 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n"
     ]
    }
   ],
   "source": [
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 431266 tokens: [(u'th', 10590), (u'alvares', 3), (u'large', 6446), (u'second', 9320), (u'new', 16522), (u'landmine', 8), (u'use', 7731), (u'peary', 14), (u'mswati', 7), (u'known', 16816)]...\n",
      "INFO : keeping 30537 tokens which were in no less than 20 and no more than 6141 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(30537 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(30537 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE_PCT)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving dictionary mapping to data/id2word_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "# save the id2word dictionary\n",
    "id2word_wiki.save_as_text(WIKI_DICT_PATH)\n",
    "\n",
    "# to reload: \n",
    "# from gensim.corpora import Dictionary\n",
    "# loaded_dct = Dictionary.load_from_text(WIKI_DICT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(WIKI_CORPUS_PATH, id2word_wiki)\n",
    "vector = next(iter(wiki_corpus))\n",
    "# print(vector)  # print the first vector in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to data/wiki_bow.mm\n",
      "INFO : saving sparse matrix to data/wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : saved 61418x30537 matrix, density=0.318% (5967192/1875521466)\n",
      "INFO : saving MmCorpus index to data/wiki_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 40s, sys: 2.13 s, total: 4min 43s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "# store bag of words of the corpus into a file\n",
    "%time gensim.corpora.MmCorpus.serialize(WIKI_BOW_FILE, wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from lda/wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from lda/wiki_bow.mm\n",
      "INFO : accepted corpus with 61418 documents, 30537 features, 5967192 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(61418 documents, 30537 features, 5967192 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "# load mm corpus\n",
    "mm_corpus = gensim.corpora.MmCorpus(WIKI_BOW_FILE)\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.01\n",
      "INFO : using symmetric eta at 3.27471591839e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 100 topics, 4 passes over the supplied corpus of 4000 documents, updating model once every 2000 documents, evaluating perplexity every 4000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : PROGRESS: pass 0, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #25 (0.010): 0.007*\"february\" + 0.004*\"hex\" + 0.004*\"mass\" + 0.003*\"rgb\" + 0.003*\"color\" + 0.003*\"november\" + 0.003*\"music\" + 0.003*\"indigo\" + 0.003*\"france\" + 0.003*\"december\"\n",
      "INFO : topic #38 (0.010): 0.004*\"jpg\" + 0.004*\"image\" + 0.004*\"league\" + 0.004*\"cell\" + 0.004*\"water\" + 0.003*\"cells\" + 0.003*\"aluminium\" + 0.003*\"food\" + 0.003*\"countries\" + 0.003*\"premier\"\n",
      "INFO : topic #17 (0.010): 0.005*\"moon\" + 0.004*\"serbia\" + 0.004*\"copper\" + 0.003*\"ii\" + 0.003*\"degree\" + 0.003*\"earth\" + 0.003*\"president\" + 0.003*\"german\" + 0.003*\"actress\" + 0.003*\"os\"\n",
      "INFO : topic #5 (0.010): 0.004*\"food\" + 0.004*\"sea\" + 0.004*\"dance\" + 0.003*\"lake\" + 0.003*\"jupiter\" + 0.002*\"water\" + 0.002*\"beach\" + 0.002*\"rain\" + 0.002*\"air\" + 0.002*\"british\"\n",
      "INFO : topic #85 (0.010): 0.005*\"earth\" + 0.005*\"finland\" + 0.004*\"water\" + 0.004*\"language\" + 0.004*\"coffee\" + 0.004*\"lake\" + 0.003*\"person\" + 0.003*\"moon\" + 0.003*\"leonardo\" + 0.003*\"words\"\n",
      "INFO : topic diff=47.595729, rho=1.000000\n",
      "INFO : -12.585 per-word bound, 6144.6 perplexity estimate based on a held-out corpus of 2000 documents with 764931 words\n",
      "INFO : PROGRESS: pass 0, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #85 (0.010): 0.010*\"halifax\" + 0.006*\"horse\" + 0.004*\"inflation\" + 0.004*\"water\" + 0.004*\"language\" + 0.004*\"duel\" + 0.004*\"finland\" + 0.003*\"country\" + 0.003*\"dutch\" + 0.003*\"earth\"\n",
      "INFO : topic #78 (0.010): 0.010*\"teeth\" + 0.009*\"lead\" + 0.007*\"disease\" + 0.007*\"steel\" + 0.006*\"things\" + 0.006*\"whale\" + 0.006*\"compounds\" + 0.005*\"metal\" + 0.005*\"kidney\" + 0.005*\"iron\"\n",
      "INFO : topic #84 (0.010): 0.015*\"sea\" + 0.008*\"languages\" + 0.006*\"moose\" + 0.005*\"language\" + 0.005*\"countries\" + 0.004*\"aramaic\" + 0.004*\"latin\" + 0.004*\"denmark\" + 0.004*\"baltic\" + 0.004*\"america\"\n",
      "INFO : topic #98 (0.010): 0.013*\"vaccines\" + 0.010*\"carbon\" + 0.010*\"odysseus\" + 0.008*\"california\" + 0.007*\"mac\" + 0.007*\"san\" + 0.007*\"released\" + 0.006*\"sparta\" + 0.005*\"sacramento\" + 0.005*\"protein\"\n",
      "INFO : topic #46 (0.010): 0.006*\"mount\" + 0.005*\"dei\" + 0.004*\"country\" + 0.004*\"government\" + 0.004*\"president\" + 0.003*\"town\" + 0.003*\"island\" + 0.003*\"robber\" + 0.003*\"weapons\" + 0.003*\"edition\"\n",
      "INFO : topic diff=11.068543, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #79 (0.010): 0.042*\"left\" + 0.040*\"align\" + 0.030*\"estimate\" + 0.014*\"census\" + 0.012*\"napoleon\" + 0.011*\"result\" + 0.011*\"britannica\" + 0.010*\"books\" + 0.007*\"bach\" + 0.007*\"henson\"\n",
      "INFO : topic #48 (0.010): 0.015*\"politician\" + 0.012*\"actor\" + 0.011*\"footballer\" + 0.009*\"italian\" + 0.009*\"player\" + 0.008*\"singer\" + 0.008*\"actress\" + 0.007*\"football\" + 0.007*\"japanese\" + 0.007*\"british\"\n",
      "INFO : topic #85 (0.010): 0.019*\"leonardo\" + 0.012*\"finland\" + 0.006*\"ferrari\" + 0.005*\"painting\" + 0.005*\"venice\" + 0.005*\"halifax\" + 0.004*\"horse\" + 0.004*\"guiana\" + 0.004*\"bar\" + 0.004*\"finnish\"\n",
      "INFO : topic #52 (0.010): 0.009*\"tea\" + 0.008*\"microsoft\" + 0.006*\"file\" + 0.005*\"language\" + 0.004*\"languages\" + 0.004*\"bone\" + 0.004*\"africa\" + 0.004*\"jpg\" + 0.004*\"bullet\" + 0.003*\"web\"\n",
      "INFO : topic #93 (0.010): 0.012*\"calvin\" + 0.012*\"australia\" + 0.009*\"mathematics\" + 0.009*\"rice\" + 0.009*\"board\" + 0.008*\"game\" + 0.008*\"party\" + 0.007*\"president\" + 0.007*\"government\" + 0.006*\"coolidge\"\n",
      "INFO : topic diff=6.402986, rho=0.500000\n",
      "INFO : -9.598 per-word bound, 775.0 perplexity estimate based on a held-out corpus of 2000 documents with 764931 words\n",
      "INFO : PROGRESS: pass 1, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #40 (0.010): 0.015*\"language\" + 0.012*\"word\" + 0.012*\"words\" + 0.009*\"example\" + 0.009*\"means\" + 0.006*\"things\" + 0.006*\"languages\" + 0.005*\"person\" + 0.005*\"software\" + 0.004*\"letters\"\n",
      "INFO : topic #77 (0.010): 0.028*\"falls\" + 0.018*\"numbers\" + 0.012*\"momentum\" + 0.012*\"wheat\" + 0.010*\"natural\" + 0.010*\"films\" + 0.009*\"universe\" + 0.008*\"money\" + 0.008*\"nitrogen\" + 0.008*\"mass\"\n",
      "INFO : topic #1 (0.010): 0.015*\"penis\" + 0.013*\"body\" + 0.011*\"man\" + 0.009*\"semen\" + 0.008*\"nude\" + 0.008*\"men\" + 0.007*\"person\" + 0.007*\"testicles\" + 0.006*\"sexual\" + 0.006*\"woman\"\n",
      "INFO : topic #74 (0.010): 0.038*\"jackson\" + 0.016*\"president\" + 0.008*\"comedy\" + 0.007*\"cornish\" + 0.007*\"playstation\" + 0.007*\"em\" + 0.007*\"utah\" + 0.006*\"roosevelt\" + 0.006*\"released\" + 0.006*\"vice\"\n",
      "INFO : topic #62 (0.010): 0.016*\"december\" + 0.015*\"ireland\" + 0.014*\"qatar\" + 0.010*\"ring\" + 0.010*\"rings\" + 0.009*\"death\" + 0.009*\"saudi\" + 0.009*\"king\" + 0.008*\"arabia\" + 0.007*\"taxes\"\n",
      "INFO : topic diff=6.442329, rho=0.500000\n",
      "INFO : PROGRESS: pass 2, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #21 (0.010): 0.011*\"verb\" + 0.011*\"object\" + 0.009*\"energy\" + 0.009*\"equation\" + 0.009*\"word\" + 0.008*\"verbs\" + 0.008*\"triangle\" + 0.007*\"example\" + 0.007*\"hd\" + 0.007*\"nixon\"\n",
      "INFO : topic #36 (0.010): 0.020*\"pakistan\" + 0.015*\"florida\" + 0.013*\"india\" + 0.009*\"country\" + 0.008*\"bangladesh\" + 0.007*\"space\" + 0.007*\"bengal\" + 0.006*\"wind\" + 0.005*\"cricket\" + 0.005*\"lebanon\"\n",
      "INFO : topic #19 (0.010): 0.012*\"independence\" + 0.009*\"afghanistan\" + 0.007*\"february\" + 0.006*\"birds\" + 0.005*\"december\" + 0.005*\"australia\" + 0.004*\"november\" + 0.004*\"president\" + 0.004*\"independent\" + 0.003*\"republic\"\n",
      "INFO : topic #28 (0.010): 0.015*\"netherlands\" + 0.012*\"caesar\" + 0.012*\"greece\" + 0.010*\"dutch\" + 0.009*\"roman\" + 0.009*\"ancient\" + 0.008*\"bc\" + 0.008*\"york\" + 0.007*\"france\" + 0.007*\"rome\"\n",
      "INFO : topic #86 (0.010): 0.020*\"austen\" + 0.020*\"sodium\" + 0.013*\"potassium\" + 0.013*\"isbn\" + 0.011*\"kenya\" + 0.010*\"oxford\" + 0.009*\"cake\" + 0.009*\"prague\" + 0.009*\"jpg\" + 0.009*\"saint\"\n",
      "INFO : topic diff=5.303009, rho=0.447214\n",
      "INFO : -9.081 per-word bound, 541.4 perplexity estimate based on a held-out corpus of 2000 documents with 764931 words\n",
      "INFO : PROGRESS: pass 2, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #43 (0.010): 0.033*\"light\" + 0.010*\"energy\" + 0.009*\"korea\" + 0.008*\"speed\" + 0.007*\"birds\" + 0.006*\"space\" + 0.005*\"earth\" + 0.005*\"turtles\" + 0.005*\"distance\" + 0.005*\"frequency\"\n",
      "INFO : topic #50 (0.010): 0.065*\"island\" + 0.023*\"pizza\" + 0.013*\"computers\" + 0.012*\"program\" + 0.012*\"code\" + 0.012*\"memory\" + 0.010*\"bread\" + 0.008*\"controller\" + 0.008*\"machine\" + 0.008*\"barbados\"\n",
      "INFO : topic #60 (0.010): 0.073*\"river\" + 0.024*\"school\" + 0.023*\"jpg\" + 0.016*\"image\" + 0.014*\"education\" + 0.013*\"schools\" + 0.011*\"rivers\" + 0.008*\"flows\" + 0.008*\"melbourne\" + 0.007*\"mississippi\"\n",
      "INFO : topic #71 (0.010): 0.014*\"person\" + 0.013*\"law\" + 0.013*\"countries\" + 0.012*\"government\" + 0.011*\"rights\" + 0.011*\"things\" + 0.010*\"laws\" + 0.009*\"money\" + 0.009*\"human\" + 0.008*\"pokémon\"\n",
      "INFO : topic #69 (0.010): 0.013*\"actor\" + 0.011*\"baltimore\" + 0.010*\"manson\" + 0.009*\"singer\" + 0.008*\"politician\" + 0.007*\"actress\" + 0.007*\"player\" + 0.007*\"writer\" + 0.007*\"german\" + 0.006*\"footballer\"\n",
      "INFO : topic diff=4.672420, rho=0.447214\n",
      "INFO : PROGRESS: pass 3, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #48 (0.010): 0.013*\"politician\" + 0.010*\"alpha\" + 0.010*\"actor\" + 0.009*\"footballer\" + 0.009*\"beta\" + 0.008*\"italian\" + 0.008*\"football\" + 0.007*\"player\" + 0.007*\"singer\" + 0.007*\"japanese\"\n",
      "INFO : topic #47 (0.010): 0.019*\"union\" + 0.017*\"soviet\" + 0.015*\"reagan\" + 0.014*\"president\" + 0.011*\"government\" + 0.009*\"countries\" + 0.007*\"country\" + 0.007*\"russia\" + 0.007*\"british\" + 0.006*\"party\"\n",
      "INFO : topic #79 (0.010): 0.056*\"left\" + 0.052*\"align\" + 0.037*\"estimate\" + 0.029*\"books\" + 0.020*\"napoleon\" + 0.016*\"census\" + 0.015*\"britannica\" + 0.014*\"result\" + 0.012*\"encyclopedia\" + 0.012*\"bach\"\n",
      "INFO : topic #60 (0.010): 0.080*\"river\" + 0.025*\"jpg\" + 0.019*\"school\" + 0.019*\"image\" + 0.014*\"education\" + 0.012*\"rivers\" + 0.009*\"schools\" + 0.009*\"flows\" + 0.009*\"devon\" + 0.008*\"mississippi\"\n",
      "INFO : topic #95 (0.010): 0.041*\"japan\" + 0.027*\"kong\" + 0.026*\"hong\" + 0.022*\"japanese\" + 0.020*\"michelangelo\" + 0.018*\"china\" + 0.009*\"island\" + 0.007*\"polo\" + 0.006*\"khan\" + 0.005*\"samoa\"\n",
      "INFO : topic diff=3.657054, rho=0.408248\n",
      "INFO : -8.832 per-word bound, 455.6 perplexity estimate based on a held-out corpus of 2000 documents with 764931 words\n",
      "INFO : PROGRESS: pass 3, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #76 (0.010): 0.028*\"ep\" + 0.019*\"william\" + 0.013*\"england\" + 0.009*\"essen\" + 0.009*\"sagan\" + 0.008*\"movies\" + 0.008*\"king\" + 0.006*\"movie\" + 0.006*\"band\" + 0.006*\"harold\"\n",
      "INFO : topic #67 (0.010): 0.013*\"female\" + 0.011*\"black\" + 0.010*\"male\" + 0.007*\"fantasy\" + 0.007*\"jpg\" + 0.006*\"birds\" + 0.006*\"wings\" + 0.006*\"care\" + 0.006*\"species\" + 0.006*\"final\"\n",
      "INFO : topic #5 (0.010): 0.015*\"chocolate\" + 0.011*\"beach\" + 0.010*\"harrison\" + 0.010*\"cream\" + 0.008*\"band\" + 0.008*\"food\" + 0.008*\"albums\" + 0.008*\"dance\" + 0.007*\"clapton\" + 0.007*\"popular\"\n",
      "INFO : topic #71 (0.010): 0.016*\"person\" + 0.014*\"countries\" + 0.014*\"law\" + 0.013*\"things\" + 0.012*\"government\" + 0.011*\"rights\" + 0.010*\"laws\" + 0.010*\"money\" + 0.009*\"human\" + 0.009*\"society\"\n",
      "INFO : topic #33 (0.010): 0.030*\"february\" + 0.025*\"november\" + 0.024*\"kingdom\" + 0.021*\"calendar\" + 0.020*\"december\" + 0.016*\"empire\" + 0.016*\"days\" + 0.015*\"scotland\" + 0.015*\"roman\" + 0.014*\"leap\"\n",
      "INFO : topic diff=3.007410, rho=0.408248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 13s, sys: 13.7 s, total: 11min 26s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)  # use fewer documents during training, LDA is slow\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=100, id2word=id2word_wiki, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving LdaState object under lda/lda_model.state, separately None\n",
      "INFO : saved lda/lda_model.state\n",
      "INFO : saving LdaModel object under lda/lda_model, separately ['expElogbeta', 'sstats']\n",
      "INFO : not storing attribute id2word\n",
      "INFO : storing np array 'expElogbeta' to lda/lda_model.expElogbeta.npy\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved lda/lda_model\n"
     ]
    }
   ],
   "source": [
    "# Save LDA model\n",
    "lda_model.save(LDA_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved LDA model\n",
    "# loaded_lda_model = LdaModel.load(LDA_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'blood', 2), (u'normally', 1), (u'produced', 1), (u'cell', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "text = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
    "\n",
    "# transform text into the bag-of-words space\n",
    "bow_vector = id2word_wiki.doc2bow(tokenize(text))\n",
    "print([(id2word_wiki[id], count) for id, count in bow_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(68, 0.85857142857131141)]\n",
      "0.016*\"disease\" + 0.016*\"person\" + 0.014*\"cells\" + 0.011*\"body\" + 0.009*\"cause\" + 0.008*\"cell\" + 0.008*\"blood\" + 0.007*\"diseases\" + 0.007*\"symptoms\" + 0.006*\"bacteria\"\n"
     ]
    }
   ],
   "source": [
    "# transform into LDA space\n",
    "lda_vector = lda_model[bow_vector]\n",
    "print(lda_vector)\n",
    "# print the document's single most prominent LDA topic\n",
    "print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
