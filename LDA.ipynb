{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIKI_CORPUS_PATH = './data/simplewiki-20171020-pages-articles-multistream.xml.bz2'\n",
    "WIKI_DICT_PATH = 'lda/id2word_wiki.txt'\n",
    "ID2WORD_JSON_PATH = 'lda/id2word_dict.json'\n",
    "WORD2ID_JSON_PATH = 'lda/word2id_dict.json'\n",
    "WIKI_BOW_FILE = 'lda/wiki_bow.mm'\n",
    "NO_BELOW = 20 # filter words that appear in less than this many documents\n",
    "NO_ABOVE_PCT = 0.1 # filter words that appear in more than this percent of documents\n",
    "NUM_DOCS_TO_TRAIN = 10000\n",
    "\n",
    "LDA_SAVE_FILE = 'lda/lda_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50 or any(title.startswith(ns + ':') for ns in ignore_namespaces):\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield title, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [u'april', u'th', u'month', u'year', u'comes', u'march', u'months', u'days', u'april', u'begins']\n",
      "August [u'august', u'aug', u'th', u'month', u'year', u'gregorian', u'calendar', u'coming', u'july', u'september']\n",
      "Art [u'painting', u'renoir', u'work', u'art', u'art', u'creative', u'activity', u'people', u'people', u'called']\n",
      "A [u'page', u'letter', u'alphabet', u'indefinite', u'article', u'article', u'grammar', u'uses', u'disambiguation', u'thumb']\n",
      "Air [u'air', u'fan', u'air', u'air', u'earth', u'atmosphere', u'air', u'mixture', u'gases', u'dust']\n",
      "Autonomous communities of Spain [u'spain', u'divided', u'parts', u'called', u'autonomous', u'communities', u'autonomous', u'means', u'autonomous', u'communities']\n",
      "Alan Turing [u'statue', u'alan', u'turing', u'rebuild', u'machine', u'alan', u'turing', u'alan', u'mathison', u'turing']\n",
      "Alanis Morissette [u'alanis', u'nadine', u'morissette', u'born', u'june', u'grammy', u'award', u'winning', u'canadian', u'american']\n"
     ]
    }
   ],
   "source": [
    "# Print the article title and its first ten tokens as an example\n",
    "stream = iter_wiki(WIKI_CORPUS_PATH)\n",
    "for title, tokens in itertools.islice(iter_wiki(WIKI_CORPUS_PATH), 8):\n",
    "    print title, tokens[:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionary id2word using wikicorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_stream = (tokens for _, tokens in iter_wiki(WIKI_CORPUS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(156775 unique tokens: [u'fawn', u'\\u03c9\\u0431\\u0440\\u0430\\u0434\\u043e\\u0432\\u0430\\u043d\\u043d\\u0430\\u0467', u'vang', u'yollar\\u0131', u'idaira']...)\n",
      "INFO : adding document #20000 to Dictionary(232594 unique tokens: [u'biennials', u'sowela', u'tsukino', u'clottes', u'refreshable']...)\n",
      "INFO : adding document #30000 to Dictionary(292328 unique tokens: [u'biennials', u'sowela', u'tsukino', u'clottes', u'klatki']...)\n",
      "INFO : adding document #40000 to Dictionary(368454 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : adding document #50000 to Dictionary(416045 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : adding document #60000 to Dictionary(454336 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n",
      "INFO : built Dictionary(461803 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...) from 61418 documents (total 13008700 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 2.13 s, total: 4min 20s\n",
      "Wall time: 4min 20s\n",
      "Dictionary(461803 unique tokens: [u'biennials', u'sowela', u'biysk', u'sermersheim', u'wooda']...)\n"
     ]
    }
   ],
   "source": [
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 431266 tokens: [(u'th', 10590), (u'alvares', 3), (u'large', 6446), (u'second', 9320), (u'new', 16522), (u'landmine', 8), (u'use', 7731), (u'peary', 14), (u'mswati', 7), (u'known', 16816)]...\n",
      "INFO : keeping 30537 tokens which were in no less than 20 and no more than 6141 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(30537 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(30537 unique tokens: [u'fawn', u'schlegel', u'sonja', u'woods', u'spiders']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE_PCT)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving dictionary mapping to data/id2word_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "# save the id2word dictionary\n",
    "id2word_wiki.save_as_text(WIKI_DICT_PATH)\n",
    "\n",
    "# to reload: \n",
    "# from gensim.corpora import Dictionary\n",
    "# loaded_dct = Dictionary.load_from_text(WIKI_DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save vocab dicts to json\n",
    "with open(WORD2ID_JSON_PATH, 'w') as fp:\n",
    "    json.dump(id2word_wiki.token2id, fp)\n",
    "    \n",
    "with open(ID2WORD_JSON_PATH, 'w') as fp:\n",
    "    json.dump(id2word_wiki.id2token, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(WIKI_CORPUS_PATH, id2word_wiki)\n",
    "vector = next(iter(wiki_corpus))\n",
    "# print(vector)  # print the first vector in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to data/wiki_bow.mm\n",
      "INFO : saving sparse matrix to data/wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : saved 61418x30537 matrix, density=0.318% (5967192/1875521466)\n",
      "INFO : saving MmCorpus index to data/wiki_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 40s, sys: 2.13 s, total: 4min 43s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "# store bag of words of the corpus into a file\n",
    "%time gensim.corpora.MmCorpus.serialize(WIKI_BOW_FILE, wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from lda/wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from lda/wiki_bow.mm\n",
      "INFO : accepted corpus with 61418 documents, 30537 features, 5967192 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(61418 documents, 30537 features, 5967192 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "# load mm corpus\n",
    "mm_corpus = gensim.corpora.MmCorpus(WIKI_BOW_FILE)\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.01\n",
      "INFO : using symmetric eta at 3.27471591839e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 100 topics, 4 passes over the supplied corpus of 10000 documents, updating model once every 2000 documents, evaluating perplexity every 10000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #2000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #55 (0.010): 0.004*\"president\" + 0.003*\"things\" + 0.003*\"countries\" + 0.003*\"person\" + 0.003*\"british\" + 0.002*\"explorer\" + 0.002*\"light\" + 0.002*\"green\" + 0.002*\"government\" + 0.002*\"open\"\n",
      "INFO : topic #52 (0.010): 0.005*\"hd\" + 0.004*\"weapons\" + 0.003*\"falls\" + 0.003*\"align\" + 0.003*\"left\" + 0.003*\"ii\" + 0.003*\"government\" + 0.003*\"country\" + 0.003*\"business\" + 0.002*\"earthquake\"\n",
      "INFO : topic #0 (0.010): 0.005*\"british\" + 0.004*\"actor\" + 0.003*\"actress\" + 0.003*\"king\" + 0.003*\"wall\" + 0.003*\"french\" + 0.003*\"writer\" + 0.003*\"politician\" + 0.003*\"president\" + 0.003*\"player\"\n",
      "INFO : topic #59 (0.010): 0.006*\"planets\" + 0.004*\"began\" + 0.004*\"days\" + 0.003*\"earth\" + 0.003*\"solar\" + 0.003*\"comedy\" + 0.003*\"moons\" + 0.003*\"government\" + 0.003*\"movie\" + 0.003*\"shinkansen\"\n",
      "INFO : topic #87 (0.010): 0.006*\"king\" + 0.005*\"black\" + 0.005*\"england\" + 0.004*\"henry\" + 0.003*\"god\" + 0.003*\"william\" + 0.003*\"elizabeth\" + 0.002*\"government\" + 0.002*\"queen\" + 0.002*\"mountain\"\n",
      "INFO : topic diff=43.715177, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #4000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #6 (0.010): 0.021*\"jackson\" + 0.011*\"usb\" + 0.010*\"cylinder\" + 0.009*\"device\" + 0.008*\"devices\" + 0.007*\"edison\" + 0.005*\"music\" + 0.005*\"potassium\" + 0.005*\"buren\" + 0.005*\"battery\"\n",
      "INFO : topic #86 (0.010): 0.008*\"blind\" + 0.008*\"animals\" + 0.007*\"blindness\" + 0.006*\"jellyfish\" + 0.005*\"british\" + 0.005*\"serfs\" + 0.005*\"england\" + 0.005*\"soil\" + 0.004*\"december\" + 0.004*\"live\"\n",
      "INFO : topic #46 (0.010): 0.012*\"light\" + 0.008*\"momentum\" + 0.006*\"length\" + 0.006*\"energy\" + 0.006*\"constant\" + 0.006*\"object\" + 0.006*\"distance\" + 0.006*\"example\" + 0.004*\"px\" + 0.004*\"brain\"\n",
      "INFO : topic #78 (0.010): 0.014*\"esperanto\" + 0.013*\"means\" + 0.013*\"language\" + 0.010*\"word\" + 0.007*\"words\" + 0.005*\"masturbation\" + 0.004*\"country\" + 0.003*\"learn\" + 0.003*\"river\" + 0.003*\"example\"\n",
      "INFO : topic #59 (0.010): 0.007*\"series\" + 0.007*\"comedy\" + 0.006*\"movies\" + 0.006*\"album\" + 0.005*\"began\" + 0.005*\"movie\" + 0.004*\"marijuana\" + 0.004*\"studio\" + 0.004*\"planets\" + 0.003*\"released\"\n",
      "INFO : topic diff=8.615756, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #6000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #94 (0.010): 0.012*\"french\" + 0.009*\"king\" + 0.009*\"writer\" + 0.008*\"singer\" + 0.008*\"actor\" + 0.007*\"france\" + 0.007*\"politician\" + 0.007*\"british\" + 0.007*\"german\" + 0.006*\"italian\"\n",
      "INFO : topic #76 (0.010): 0.025*\"blood\" + 0.019*\"body\" + 0.017*\"person\" + 0.014*\"asthma\" + 0.011*\"organ\" + 0.009*\"heart\" + 0.007*\"organs\" + 0.007*\"makes\" + 0.007*\"cells\" + 0.005*\"stops\"\n",
      "INFO : topic #60 (0.010): 0.018*\"harry\" + 0.008*\"stitch\" + 0.008*\"book\" + 0.007*\"witches\" + 0.006*\"tales\" + 0.006*\"magic\" + 0.006*\"lilo\" + 0.006*\"horse\" + 0.005*\"books\" + 0.005*\"school\"\n",
      "INFO : topic #32 (0.010): 0.018*\"february\" + 0.015*\"calendar\" + 0.015*\"earth\" + 0.013*\"november\" + 0.013*\"moon\" + 0.012*\"sun\" + 0.012*\"melbourne\" + 0.009*\"hemisphere\" + 0.008*\"windows\" + 0.007*\"microsoft\"\n",
      "INFO : topic #49 (0.010): 0.010*\"earth\" + 0.010*\"pokémon\" + 0.008*\"space\" + 0.007*\"light\" + 0.007*\"energy\" + 0.005*\"array\" + 0.005*\"magnetic\" + 0.005*\"fat\" + 0.005*\"star\" + 0.004*\"gas\"\n",
      "INFO : topic diff=6.696091, rho=0.577350\n",
      "INFO : PROGRESS: pass 0, at document #8000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #37 (0.010): 0.017*\"water\" + 0.012*\"cells\" + 0.007*\"cell\" + 0.007*\"body\" + 0.004*\"leaves\" + 0.004*\"person\" + 0.004*\"common\" + 0.004*\"inside\" + 0.004*\"food\" + 0.003*\"ice\"\n",
      "INFO : topic #82 (0.010): 0.021*\"spider\" + 0.019*\"pinocchio\" + 0.018*\"terminal\" + 0.017*\"spiders\" + 0.012*\"sperm\" + 0.011*\"random\" + 0.010*\"classics\" + 0.009*\"zimbabwe\" + 0.008*\"canyon\" + 0.007*\"bbc\"\n",
      "INFO : topic #39 (0.010): 0.038*\"air\" + 0.007*\"ground\" + 0.007*\"aaliyah\" + 0.007*\"trees\" + 0.006*\"water\" + 0.006*\"tropical\" + 0.005*\"drives\" + 0.005*\"weather\" + 0.004*\"power\" + 0.004*\"pressure\"\n",
      "INFO : topic #12 (0.010): 0.014*\"atoms\" + 0.013*\"atom\" + 0.012*\"force\" + 0.011*\"mass\" + 0.010*\"reconnaissance\" + 0.010*\"particles\" + 0.009*\"atomic\" + 0.009*\"electrons\" + 0.009*\"charge\" + 0.008*\"element\"\n",
      "INFO : topic #83 (0.010): 0.034*\"nuremberg\" + 0.013*\"intestine\" + 0.009*\"fertilization\" + 0.009*\"wesleyan\" + 0.008*\"groening\" + 0.007*\"raider\" + 0.006*\"parasitic\" + 0.006*\"que\" + 0.005*\"tower\" + 0.005*\"handler\"\n",
      "INFO : topic diff=6.097963, rho=0.500000\n",
      "INFO : -10.304 per-word bound, 1264.2 perplexity estimate based on a held-out corpus of 2000 documents with 326916 words\n",
      "INFO : PROGRESS: pass 0, at document #10000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #46 (0.010): 0.050*\"px\" + 0.017*\"light\" + 0.009*\"field\" + 0.009*\"electric\" + 0.008*\"example\" + 0.008*\"object\" + 0.007*\"distance\" + 0.007*\"energy\" + 0.006*\"unit\" + 0.006*\"length\"\n",
      "INFO : topic #54 (0.010): 0.035*\"school\" + 0.014*\"education\" + 0.014*\"prize\" + 0.013*\"nobel\" + 0.012*\"court\" + 0.010*\"ceramics\" + 0.010*\"students\" + 0.009*\"college\" + 0.009*\"law\" + 0.009*\"usa\"\n",
      "INFO : topic #13 (0.010): 0.032*\"visa\" + 0.025*\"council\" + 0.024*\"lord\" + 0.013*\"european\" + 0.011*\"sandwich\" + 0.009*\"member\" + 0.008*\"ring\" + 0.007*\"hobbit\" + 0.007*\"countries\" + 0.006*\"cricket\"\n",
      "INFO : topic #22 (0.010): 0.032*\"japan\" + 0.020*\"japanese\" + 0.016*\"empire\" + 0.013*\"period\" + 0.009*\"inca\" + 0.007*\"century\" + 0.007*\"emperor\" + 0.007*\"imperial\" + 0.005*\"greece\" + 0.005*\"visual\"\n",
      "INFO : topic #24 (0.010): 0.059*\"wing\" + 0.024*\"plane\" + 0.015*\"bear\" + 0.014*\"hampshire\" + 0.014*\"arizona\" + 0.013*\"wwe\" + 0.012*\"arms\" + 0.011*\"image\" + 0.011*\"coat\" + 0.010*\"phoenix\"\n",
      "INFO : topic diff=5.342125, rho=0.447214\n",
      "INFO : PROGRESS: pass 1, at document #2000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #36 (0.010): 0.011*\"example\" + 0.009*\"prussia\" + 0.008*\"programming\" + 0.008*\"verbs\" + 0.007*\"verb\" + 0.006*\"languages\" + 0.006*\"language\" + 0.006*\"person\" + 0.006*\"sentence\" + 0.005*\"word\"\n",
      "INFO : topic #19 (0.010): 0.016*\"falls\" + 0.015*\"napoleon\" + 0.014*\"apple\" + 0.010*\"france\" + 0.008*\"algebra\" + 0.008*\"french\" + 0.008*\"mathematics\" + 0.008*\"ipod\" + 0.008*\"paris\" + 0.007*\"basque\"\n",
      "INFO : topic #78 (0.010): 0.034*\"language\" + 0.031*\"word\" + 0.025*\"words\" + 0.013*\"means\" + 0.011*\"languages\" + 0.009*\"meaning\" + 0.009*\"example\" + 0.009*\"alphabet\" + 0.008*\"pakistan\" + 0.007*\"chinese\"\n",
      "INFO : topic #22 (0.010): 0.039*\"japan\" + 0.024*\"japanese\" + 0.018*\"empire\" + 0.011*\"greece\" + 0.010*\"period\" + 0.008*\"century\" + 0.008*\"visual\" + 0.006*\"inca\" + 0.006*\"emperor\" + 0.006*\"tokyo\"\n",
      "INFO : topic #58 (0.010): 0.032*\"movie\" + 0.031*\"award\" + 0.019*\"film\" + 0.012*\"actress\" + 0.011*\"actor\" + 0.011*\"movies\" + 0.011*\"awards\" + 0.009*\"role\" + 0.009*\"nominated\" + 0.008*\"episode\"\n",
      "INFO : topic diff=4.258501, rho=0.377964\n",
      "INFO : PROGRESS: pass 1, at document #4000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #5 (0.010): 0.010*\"player\" + 0.009*\"german\" + 0.009*\"politician\" + 0.008*\"actor\" + 0.008*\"french\" + 0.007*\"russian\" + 0.007*\"singer\" + 0.006*\"hungary\" + 0.006*\"austria\" + 0.006*\"writer\"\n",
      "INFO : topic #83 (0.010): 0.023*\"nuremberg\" + 0.018*\"fertilization\" + 0.017*\"simpsons\" + 0.016*\"trek\" + 0.016*\"intestine\" + 0.012*\"handler\" + 0.012*\"fake\" + 0.011*\"gut\" + 0.010*\"lemony\" + 0.010*\"myung\"\n",
      "INFO : topic #48 (0.010): 0.029*\"metal\" + 0.024*\"steel\" + 0.024*\"iron\" + 0.014*\"metals\" + 0.013*\"radio\" + 0.007*\"aluminium\" + 0.006*\"readings\" + 0.006*\"bond\" + 0.005*\"copper\" + 0.005*\"black\"\n",
      "INFO : topic #8 (0.010): 0.028*\"nuclear\" + 0.023*\"singapore\" + 0.012*\"indonesia\" + 0.011*\"radiation\" + 0.010*\"hail\" + 0.010*\"wilmington\" + 0.008*\"malay\" + 0.007*\"wake\" + 0.007*\"irrigation\" + 0.006*\"field\"\n",
      "INFO : topic #65 (0.010): 0.035*\"woman\" + 0.031*\"sex\" + 0.028*\"women\" + 0.021*\"sexual\" + 0.016*\"men\" + 0.014*\"man\" + 0.013*\"britannica\" + 0.012*\"tea\" + 0.011*\"hiv\" + 0.010*\"sexually\"\n",
      "INFO : topic diff=3.918910, rho=0.377964\n",
      "INFO : PROGRESS: pass 1, at document #6000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #71 (0.010): 0.030*\"anime\" + 0.021*\"president\" + 0.018*\"manga\" + 0.014*\"gagarin\" + 0.013*\"adams\" + 0.013*\"money\" + 0.012*\"price\" + 0.011*\"godzilla\" + 0.010*\"poker\" + 0.010*\"roosevelt\"\n",
      "INFO : topic #92 (0.010): 0.056*\"party\" + 0.021*\"communist\" + 0.020*\"republic\" + 0.020*\"union\" + 0.017*\"germany\" + 0.017*\"flag\" + 0.017*\"soviet\" + 0.015*\"socialist\" + 0.011*\"serbia\" + 0.010*\"country\"\n",
      "INFO : topic #33 (0.010): 0.013*\"german\" + 0.012*\"italian\" + 0.010*\"british\" + 0.010*\"french\" + 0.008*\"politician\" + 0.006*\"actor\" + 0.006*\"footballer\" + 0.006*\"writer\" + 0.006*\"france\" + 0.006*\"actress\"\n",
      "INFO : topic #97 (0.010): 0.032*\"birds\" + 0.023*\"order\" + 0.020*\"bones\" + 0.018*\"mammals\" + 0.017*\"wings\" + 0.016*\"bats\" + 0.014*\"skin\" + 0.013*\"sharks\" + 0.012*\"tail\" + 0.012*\"snakes\"\n",
      "INFO : topic #13 (0.010): 0.025*\"council\" + 0.024*\"lord\" + 0.021*\"cricket\" + 0.019*\"european\" + 0.012*\"member\" + 0.010*\"grand\" + 0.010*\"visa\" + 0.010*\"ring\" + 0.010*\"theft\" + 0.010*\"members\"\n",
      "INFO : topic diff=3.534806, rho=0.377964\n",
      "INFO : PROGRESS: pass 1, at document #8000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #46 (0.010): 0.046*\"px\" + 0.017*\"light\" + 0.010*\"field\" + 0.010*\"energy\" + 0.009*\"electric\" + 0.009*\"magnetic\" + 0.008*\"object\" + 0.008*\"distance\" + 0.008*\"example\" + 0.007*\"unit\"\n",
      "INFO : topic #3 (0.010): 0.016*\"band\" + 0.016*\"vocals\" + 0.012*\"guitar\" + 0.012*\"love\" + 0.009*\"album\" + 0.008*\"released\" + 0.008*\"bass\" + 0.007*\"drums\" + 0.006*\"song\" + 0.006*\"members\"\n",
      "INFO : topic #62 (0.010): 0.060*\"island\" + 0.030*\"islands\" + 0.017*\"saint\" + 0.012*\"sea\" + 0.010*\"ocean\" + 0.008*\"coast\" + 0.007*\"st\" + 0.007*\"east\" + 0.007*\"pacific\" + 0.006*\"atlantic\"\n",
      "INFO : topic #50 (0.010): 0.045*\"game\" + 0.037*\"mario\" + 0.024*\"games\" + 0.021*\"nintendo\" + 0.017*\"solid\" + 0.017*\"super\" + 0.016*\"video\" + 0.013*\"boy\" + 0.011*\"arsenic\" + 0.011*\"zelda\"\n",
      "INFO : topic #66 (0.010): 0.041*\"carbon\" + 0.024*\"macbeth\" + 0.023*\"gas\" + 0.022*\"dioxide\" + 0.022*\"sodium\" + 0.015*\"oil\" + 0.015*\"coal\" + 0.014*\"acid\" + 0.012*\"gases\" + 0.011*\"hydrogen\"\n",
      "INFO : topic diff=3.192558, rho=0.377964\n",
      "INFO : -9.186 per-word bound, 582.3 perplexity estimate based on a held-out corpus of 2000 documents with 326916 words\n",
      "INFO : PROGRESS: pass 1, at document #10000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #37 (0.010): 0.025*\"water\" + 0.014*\"cells\" + 0.012*\"cell\" + 0.006*\"body\" + 0.005*\"leaves\" + 0.005*\"plants\" + 0.005*\"inside\" + 0.005*\"food\" + 0.005*\"types\" + 0.005*\"plant\"\n",
      "INFO : topic #53 (0.010): 0.148*\"river\" + 0.030*\"wave\" + 0.021*\"boys\" + 0.021*\"water\" + 0.019*\"flows\" + 0.015*\"rivers\" + 0.014*\"girls\" + 0.013*\"waves\" + 0.008*\"boy\" + 0.008*\"longest\"\n",
      "INFO : topic #44 (0.010): 0.028*\"food\" + 0.015*\"plants\" + 0.013*\"animals\" + 0.010*\"species\" + 0.010*\"plant\" + 0.010*\"humans\" + 0.009*\"natural\" + 0.009*\"human\" + 0.007*\"evolution\" + 0.007*\"bread\"\n",
      "INFO : topic #55 (0.010): 0.026*\"license\" + 0.022*\"professor\" + 0.017*\"spd\" + 0.016*\"chancellor\" + 0.015*\"rats\" + 0.014*\"cdu\" + 0.012*\"germany\" + 0.011*\"chun\" + 0.010*\"social\" + 0.009*\"consumers\"\n",
      "INFO : topic #0 (0.010): 0.095*\"air\" + 0.091*\"airlines\" + 0.040*\"airways\" + 0.034*\"boeing\" + 0.023*\"wall\" + 0.011*\"express\" + 0.011*\"airline\" + 0.010*\"international\" + 0.010*\"aviation\" + 0.009*\"cargo\"\n",
      "INFO : topic diff=2.793327, rho=0.377964\n",
      "INFO : PROGRESS: pass 2, at document #2000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #87 (0.010): 0.018*\"king\" + 0.011*\"england\" + 0.009*\"father\" + 0.009*\"son\" + 0.008*\"married\" + 0.007*\"death\" + 0.007*\"henry\" + 0.007*\"children\" + 0.006*\"wife\" + 0.006*\"queen\"\n",
      "INFO : topic #16 (0.010): 0.012*\"hindu\" + 0.010*\"rural\" + 0.010*\"india\" + 0.006*\"hindus\" + 0.006*\"saxony\" + 0.006*\"district\" + 0.006*\"rule\" + 0.005*\"hinduism\" + 0.005*\"yoga\" + 0.005*\"tagore\"\n",
      "INFO : topic #28 (0.010): 0.045*\"spanish\" + 0.030*\"america\" + 0.022*\"latin\" + 0.020*\"spain\" + 0.017*\"chile\" + 0.014*\"native\" + 0.014*\"cuba\" + 0.014*\"colombia\" + 0.014*\"argentina\" + 0.011*\"venezuela\"\n",
      "INFO : topic #46 (0.010): 0.028*\"px\" + 0.018*\"light\" + 0.013*\"energy\" + 0.010*\"object\" + 0.010*\"units\" + 0.010*\"distance\" + 0.009*\"unit\" + 0.009*\"speed\" + 0.008*\"field\" + 0.008*\"length\"\n",
      "INFO : topic #48 (0.010): 0.052*\"metal\" + 0.029*\"iron\" + 0.024*\"steel\" + 0.022*\"metals\" + 0.016*\"copper\" + 0.011*\"bond\" + 0.011*\"aluminium\" + 0.010*\"gold\" + 0.008*\"radio\" + 0.007*\"tin\"\n",
      "INFO : topic diff=2.381120, rho=0.353553\n",
      "INFO : PROGRESS: pass 2, at document #4000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #11 (0.010): 0.019*\"study\" + 0.016*\"things\" + 0.016*\"science\" + 0.012*\"theory\" + 0.012*\"person\" + 0.011*\"philosophy\" + 0.010*\"ideas\" + 0.009*\"think\" + 0.008*\"human\" + 0.008*\"mind\"\n",
      "INFO : topic #46 (0.010): 0.027*\"px\" + 0.017*\"light\" + 0.012*\"energy\" + 0.010*\"object\" + 0.010*\"distance\" + 0.009*\"units\" + 0.009*\"speed\" + 0.009*\"unit\" + 0.008*\"length\" + 0.008*\"point\"\n",
      "INFO : topic #49 (0.010): 0.022*\"earth\" + 0.014*\"space\" + 0.012*\"universe\" + 0.012*\"light\" + 0.011*\"energy\" + 0.011*\"stars\" + 0.011*\"sun\" + 0.010*\"star\" + 0.010*\"planet\" + 0.009*\"matter\"\n",
      "INFO : topic #91 (0.010): 0.016*\"actor\" + 0.016*\"actress\" + 0.014*\"singer\" + 0.012*\"politician\" + 0.012*\"footballer\" + 0.012*\"president\" + 0.010*\"player\" + 0.010*\"writer\" + 0.009*\"british\" + 0.009*\"german\"\n",
      "INFO : topic #35 (0.010): 0.013*\"person\" + 0.011*\"wear\" + 0.009*\"paper\" + 0.007*\"clothing\" + 0.007*\"certain\" + 0.007*\"example\" + 0.006*\"clothes\" + 0.006*\"worn\" + 0.006*\"types\" + 0.005*\"plastic\"\n",
      "INFO : topic diff=1.979321, rho=0.353553\n",
      "INFO : PROGRESS: pass 2, at document #6000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #47 (0.010): 0.147*\"bc\" + 0.050*\"century\" + 0.017*\"ancient\" + 0.014*\"france\" + 0.012*\"dover\" + 0.012*\"channel\" + 0.011*\"french\" + 0.010*\"romans\" + 0.010*\"ad\" + 0.009*\"inch\"\n",
      "INFO : topic #9 (0.010): 0.035*\"png\" + 0.017*\"painting\" + 0.017*\"art\" + 0.014*\"paint\" + 0.013*\"painted\" + 0.012*\"paintings\" + 0.011*\"michelangelo\" + 0.011*\"image\" + 0.009*\"famous\" + 0.008*\"florence\"\n",
      "INFO : topic #41 (0.010): 0.039*\"al\" + 0.025*\"link\" + 0.014*\"muslims\" + 0.014*\"muhammad\" + 0.013*\"abortion\" + 0.009*\"god\" + 0.009*\"islamic\" + 0.009*\"islam\" + 0.008*\"muslim\" + 0.007*\"canadiens\"\n",
      "INFO : topic #73 (0.010): 0.175*\"squadron\" + 0.040*\"fighter\" + 0.032*\"playstation\" + 0.019*\"xbox\" + 0.017*\"st\" + 0.016*\"sony\" + 0.012*\"kyoto\" + 0.011*\"air\" + 0.010*\"tactical\" + 0.010*\"fireworks\"\n",
      "INFO : topic #96 (0.010): 0.043*\"german\" + 0.035*\"germany\" + 0.030*\"berlin\" + 0.016*\"east\" + 0.012*\"normandy\" + 0.011*\"germans\" + 0.010*\"salt\" + 0.010*\"wall\" + 0.009*\"ep\" + 0.009*\"hamilton\"\n",
      "INFO : topic diff=1.629192, rho=0.353553\n",
      "INFO : PROGRESS: pass 2, at document #8000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #72 (0.010): 0.043*\"games\" + 0.042*\"game\" + 0.032*\"player\" + 0.020*\"olympic\" + 0.019*\"ball\" + 0.017*\"players\" + 0.017*\"windows\" + 0.011*\"play\" + 0.010*\"zeus\" + 0.010*\"played\"\n",
      "INFO : topic #73 (0.010): 0.396*\"squadron\" + 0.085*\"fighter\" + 0.033*\"st\" + 0.031*\"air\" + 0.017*\"test\" + 0.017*\"tactical\" + 0.012*\"operations\" + 0.011*\"missile\" + 0.011*\"support\" + 0.011*\"space\"\n",
      "INFO : topic #49 (0.010): 0.021*\"earth\" + 0.015*\"space\" + 0.012*\"star\" + 0.011*\"energy\" + 0.011*\"light\" + 0.011*\"stars\" + 0.010*\"planet\" + 0.010*\"sun\" + 0.010*\"universe\" + 0.009*\"planets\"\n",
      "INFO : topic #35 (0.010): 0.017*\"raid\" + 0.012*\"person\" + 0.012*\"wear\" + 0.007*\"paper\" + 0.007*\"clothing\" + 0.007*\"certain\" + 0.006*\"worn\" + 0.006*\"protect\" + 0.006*\"example\" + 0.006*\"plastic\"\n",
      "INFO : topic #19 (0.010): 0.037*\"napoleon\" + 0.021*\"apple\" + 0.019*\"ipod\" + 0.018*\"paris\" + 0.016*\"generation\" + 0.015*\"falls\" + 0.014*\"french\" + 0.013*\"france\" + 0.011*\"gb\" + 0.010*\"theorem\"\n",
      "INFO : topic diff=1.405886, rho=0.353553\n",
      "INFO : -8.884 per-word bound, 472.4 perplexity estimate based on a held-out corpus of 2000 documents with 326916 words\n",
      "INFO : PROGRESS: pass 2, at document #10000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #58 (0.010): 0.040*\"movie\" + 0.028*\"award\" + 0.023*\"film\" + 0.016*\"movies\" + 0.012*\"episode\" + 0.011*\"awards\" + 0.010*\"actor\" + 0.009*\"director\" + 0.009*\"role\" + 0.008*\"television\"\n",
      "INFO : topic #99 (0.010): 0.196*\"line\" + 0.046*\"lines\" + 0.035*\"opened\" + 0.019*\"trains\" + 0.018*\"stations\" + 0.017*\"fairy\" + 0.013*\"din\" + 0.012*\"zinc\" + 0.012*\"piccadilly\" + 0.011*\"underground\"\n",
      "INFO : topic #80 (0.010): 0.031*\"india\" + 0.022*\"bombardment\" + 0.015*\"british\" + 0.015*\"indian\" + 0.011*\"army\" + 0.011*\"air\" + 0.010*\"military\" + 0.009*\"airlift\" + 0.008*\"navy\" + 0.008*\"emblem\"\n",
      "INFO : topic #86 (0.010): 0.026*\"ecosystem\" + 0.023*\"soil\" + 0.021*\"blind\" + 0.019*\"braille\" + 0.019*\"frogs\" + 0.016*\"squid\" + 0.016*\"bedford\" + 0.014*\"snails\" + 0.011*\"curse\" + 0.011*\"surrey\"\n",
      "INFO : topic #78 (0.010): 0.054*\"language\" + 0.042*\"word\" + 0.025*\"words\" + 0.022*\"languages\" + 0.017*\"means\" + 0.013*\"meaning\" + 0.011*\"spoken\" + 0.011*\"alphabet\" + 0.010*\"example\" + 0.009*\"letters\"\n",
      "INFO : topic diff=1.178154, rho=0.353553\n",
      "INFO : PROGRESS: pass 3, at document #2000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #47 (0.010): 0.162*\"bc\" + 0.061*\"century\" + 0.023*\"ancient\" + 0.017*\"france\" + 0.013*\"romans\" + 0.012*\"greek\" + 0.010*\"french\" + 0.010*\"ad\" + 0.010*\"roman\" + 0.009*\"channel\"\n",
      "INFO : topic #28 (0.010): 0.050*\"spanish\" + 0.034*\"america\" + 0.027*\"spain\" + 0.020*\"latin\" + 0.019*\"chile\" + 0.018*\"cuba\" + 0.016*\"argentina\" + 0.015*\"native\" + 0.015*\"colombia\" + 0.013*\"la\"\n",
      "INFO : topic #8 (0.010): 0.076*\"nuclear\" + 0.055*\"singapore\" + 0.026*\"indonesia\" + 0.022*\"malay\" + 0.015*\"malaysia\" + 0.014*\"engineering\" + 0.013*\"radiation\" + 0.013*\"hail\" + 0.011*\"indonesian\" + 0.010*\"field\"\n",
      "INFO : topic #79 (0.010): 0.020*\"town\" + 0.017*\"capital\" + 0.016*\"county\" + 0.015*\"cities\" + 0.013*\"largest\" + 0.012*\"center\" + 0.010*\"population\" + 0.010*\"built\" + 0.010*\"famous\" + 0.009*\"east\"\n",
      "INFO : topic #6 (0.010): 0.016*\"internet\" + 0.015*\"computers\" + 0.015*\"data\" + 0.014*\"software\" + 0.013*\"windows\" + 0.011*\"microsoft\" + 0.008*\"program\" + 0.008*\"operating\" + 0.008*\"version\" + 0.008*\"systems\"\n",
      "INFO : topic diff=1.061858, rho=0.333333\n",
      "INFO : PROGRESS: pass 3, at document #4000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #61 (0.010): 0.021*\"government\" + 0.017*\"rights\" + 0.016*\"political\" + 0.012*\"law\" + 0.010*\"laws\" + 0.009*\"constitution\" + 0.008*\"power\" + 0.007*\"economic\" + 0.007*\"parliament\" + 0.007*\"country\"\n",
      "INFO : topic #97 (0.010): 0.039*\"birds\" + 0.021*\"order\" + 0.019*\"mammals\" + 0.018*\"teeth\" + 0.017*\"bones\" + 0.017*\"skin\" + 0.015*\"wings\" + 0.015*\"bird\" + 0.014*\"tail\" + 0.014*\"head\"\n",
      "INFO : topic #10 (0.010): 0.030*\"wrote\" + 0.028*\"music\" + 0.026*\"opera\" + 0.019*\"piano\" + 0.015*\"composers\" + 0.012*\"works\" + 0.012*\"famous\" + 0.012*\"composer\" + 0.010*\"operas\" + 0.010*\"century\"\n",
      "INFO : topic #32 (0.010): 0.083*\"december\" + 0.081*\"february\" + 0.072*\"november\" + 0.021*\"calendar\" + 0.015*\"days\" + 0.014*\"month\" + 0.014*\"earth\" + 0.014*\"moon\" + 0.013*\"sun\" + 0.012*\"week\"\n",
      "INFO : topic #14 (0.010): 0.056*\"ireland\" + 0.044*\"house\" + 0.028*\"taiwan\" + 0.024*\"irish\" + 0.022*\"der\" + 0.018*\"county\" + 0.017*\"die\" + 0.015*\"counties\" + 0.013*\"republic\" + 0.012*\"northern\"\n",
      "INFO : topic diff=0.846760, rho=0.333333\n",
      "INFO : PROGRESS: pass 3, at document #6000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #58 (0.010): 0.038*\"movie\" + 0.026*\"award\" + 0.019*\"film\" + 0.015*\"movies\" + 0.014*\"awards\" + 0.012*\"actor\" + 0.010*\"episode\" + 0.010*\"won\" + 0.010*\"role\" + 0.009*\"nominated\"\n",
      "INFO : topic #6 (0.010): 0.015*\"data\" + 0.014*\"windows\" + 0.013*\"internet\" + 0.013*\"computers\" + 0.012*\"software\" + 0.010*\"microsoft\" + 0.008*\"operating\" + 0.008*\"systems\" + 0.008*\"version\" + 0.008*\"information\"\n",
      "INFO : topic #54 (0.010): 0.064*\"school\" + 0.026*\"prize\" + 0.023*\"nobel\" + 0.022*\"college\" + 0.020*\"schools\" + 0.020*\"education\" + 0.019*\"students\" + 0.013*\"court\" + 0.011*\"degree\" + 0.010*\"peace\"\n",
      "INFO : topic #10 (0.010): 0.029*\"wrote\" + 0.028*\"music\" + 0.024*\"opera\" + 0.018*\"piano\" + 0.014*\"composers\" + 0.013*\"famous\" + 0.013*\"works\" + 0.011*\"composer\" + 0.010*\"century\" + 0.009*\"orchestra\"\n",
      "INFO : topic #34 (0.010): 0.015*\"rock\" + 0.010*\"rocks\" + 0.009*\"penis\" + 0.008*\"surface\" + 0.007*\"earth\" + 0.006*\"testicles\" + 0.005*\"sea\" + 0.005*\"nude\" + 0.005*\"puberty\" + 0.005*\"ago\"\n",
      "INFO : topic diff=0.675262, rho=0.333333\n",
      "INFO : PROGRESS: pass 3, at document #8000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #34 (0.010): 0.014*\"rock\" + 0.010*\"rocks\" + 0.008*\"penis\" + 0.008*\"surface\" + 0.007*\"earth\" + 0.005*\"sea\" + 0.005*\"inside\" + 0.005*\"shell\" + 0.005*\"kw\" + 0.005*\"plate\"\n",
      "INFO : topic #59 (0.010): 0.046*\"series\" + 0.039*\"television\" + 0.020*\"space\" + 0.011*\"apollo\" + 0.010*\"shows\" + 0.010*\"news\" + 0.009*\"comedy\" + 0.009*\"radio\" + 0.008*\"science\" + 0.008*\"bbc\"\n",
      "INFO : topic #11 (0.010): 0.017*\"study\" + 0.017*\"things\" + 0.014*\"person\" + 0.014*\"science\" + 0.014*\"theory\" + 0.011*\"human\" + 0.011*\"ideas\" + 0.009*\"philosophy\" + 0.008*\"idea\" + 0.008*\"think\"\n",
      "INFO : topic #22 (0.010): 0.055*\"japan\" + 0.035*\"empire\" + 0.030*\"japanese\" + 0.017*\"period\" + 0.012*\"century\" + 0.011*\"greece\" + 0.009*\"power\" + 0.009*\"emperor\" + 0.008*\"tokyo\" + 0.007*\"rulers\"\n",
      "INFO : topic #13 (0.010): 0.054*\"lord\" + 0.042*\"ring\" + 0.038*\"rings\" + 0.034*\"council\" + 0.029*\"european\" + 0.019*\"member\" + 0.019*\"cricket\" + 0.013*\"countries\" + 0.013*\"members\" + 0.010*\"eu\"\n",
      "INFO : topic diff=0.601439, rho=0.333333\n",
      "INFO : -8.761 per-word bound, 433.8 perplexity estimate based on a held-out corpus of 2000 documents with 326916 words\n",
      "INFO : PROGRESS: pass 3, at document #10000/10000\n",
      "INFO : merging changes from 2000 documents into a model of 10000 documents\n",
      "INFO : topic #32 (0.010): 0.086*\"december\" + 0.073*\"february\" + 0.069*\"november\" + 0.017*\"calendar\" + 0.014*\"earth\" + 0.013*\"sun\" + 0.012*\"days\" + 0.012*\"circle\" + 0.012*\"moon\" + 0.010*\"month\"\n",
      "INFO : topic #91 (0.010): 0.015*\"actor\" + 0.014*\"actress\" + 0.013*\"singer\" + 0.011*\"politician\" + 0.011*\"footballer\" + 0.011*\"president\" + 0.010*\"player\" + 0.009*\"writer\" + 0.009*\"british\" + 0.008*\"german\"\n",
      "INFO : topic #25 (0.010): 0.060*\"party\" + 0.020*\"democratic\" + 0.017*\"election\" + 0.015*\"message\" + 0.014*\"republican\" + 0.014*\"liberal\" + 0.014*\"conservative\" + 0.013*\"senate\" + 0.013*\"political\" + 0.012*\"house\"\n",
      "INFO : topic #64 (0.010): 0.049*\"numbers\" + 0.030*\"example\" + 0.020*\"value\" + 0.019*\"set\" + 0.018*\"scale\" + 0.017*\"written\" + 0.016*\"note\" + 0.013*\"real\" + 0.012*\"natural\" + 0.010*\"means\"\n",
      "INFO : topic #53 (0.010): 0.162*\"river\" + 0.041*\"water\" + 0.029*\"wave\" + 0.022*\"rivers\" + 0.019*\"flows\" + 0.017*\"waves\" + 0.013*\"stream\" + 0.013*\"lake\" + 0.013*\"valley\" + 0.012*\"flow\"\n",
      "INFO : topic diff=0.513752, rho=0.333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 19s, sys: 8.42 s, total: 12min 27s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, NUM_DOCS_TO_TRAIN)  # use fewer documents during training, LDA is slow\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=100, id2word=id2word_wiki, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving LdaState object under lda/lda_model.state, separately None\n",
      "INFO : saved lda/lda_model.state\n",
      "INFO : saving LdaModel object under lda/lda_model, separately ['expElogbeta', 'sstats']\n",
      "INFO : not storing attribute id2word\n",
      "INFO : storing np array 'expElogbeta' to lda/lda_model.expElogbeta.npy\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved lda/lda_model\n"
     ]
    }
   ],
   "source": [
    "# Save LDA model\n",
    "lda_model.save(LDA_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved LDA model\n",
    "# loaded_lda_model = LdaModel.load(LDA_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'blood', 2), (u'normally', 1), (u'produced', 1), (u'cell', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "text = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
    "\n",
    "# transform text into the bag-of-words space\n",
    "bow_vector = id2word_wiki.doc2bow(tokenize(text))\n",
    "print([(id2word_wiki[id], count) for id, count in bow_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(68, 0.85857142857131141)]\n",
      "0.016*\"disease\" + 0.016*\"person\" + 0.014*\"cells\" + 0.011*\"body\" + 0.009*\"cause\" + 0.008*\"cell\" + 0.008*\"blood\" + 0.007*\"diseases\" + 0.007*\"symptoms\" + 0.006*\"bacteria\"\n"
     ]
    }
   ],
   "source": [
    "# transform into LDA space\n",
    "lda_vector = lda_model[bow_vector]\n",
    "print(lda_vector)\n",
    "# print the document's single most prominent LDA topic\n",
    "print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'blood', 2), (u'normally', 1), (u'produced', 1), (u'cell', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "text = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
    "\n",
    "# transform text into the bag-of-words space\n",
    "bow_vector = id2word_wiki.doc2bow(tokenize(text))\n",
    "print([(id2word_wiki[id], count) for id, count in bow_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(37, 0.48440212237544455), (76, 0.37559787762455704)]\n",
      "0.024*\"water\" + 0.018*\"cells\" + 0.016*\"cell\" + 0.006*\"inside\" + 0.006*\"plants\" + 0.006*\"plant\" + 0.006*\"types\" + 0.005*\"food\" + 0.005*\"lungs\" + 0.005*\"leaves\"\n"
     ]
    }
   ],
   "source": [
    "# transform into LDA space\n",
    "lda_vector = lda_model[bow_vector]\n",
    "print(lda_vector)\n",
    "# print the document's single most prominent LDA topic\n",
    "print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
